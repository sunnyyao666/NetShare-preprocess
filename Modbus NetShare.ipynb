{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NetShare Modbus pipeline\n",
    "\n",
    "A guide on how to run the whole NetShare pipeline from a `PCAP` file containing modbus packets to final generated synthetic results on using your own machine, so that you can fully utilize the capability of the machine and customize the pipeline as you wanted.\n",
    "\n",
    "Notice that the pipeline needs to be installed and configured on the machine to be used, so if you only want a quick demo, we recommend you to use the Google Colab version [here](https://colab.research.google.com/drive/10EzjAZOl6CPelxFswphxbBDvsAQtoReF#scrollTo=uYlAIox_Y00X); if you only want to use the standard NetShare pipeline, we recommend you to use the web version here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install environment\n",
    "\n",
    "Before you start to use NetShare, you will need to install the required environment. Because NetShare is recommended to be installed in Anaconda, and you are using Jupyter Notebook, so first you need to install the environment **outside** this notebook, and then run this notebook inside that environment to run the code in the notebook.\n",
    "\n",
    "First, we assume that you have Anaconda installed. Else, please see [here](https://www.anaconda.com/).\n",
    "\n",
    "Now, create a new virtual environment in Anaconda using this following command. Once again, run these commands **outside** this notebook on your own machine.\n",
    "```bash\n",
    "conda create --name NetShare python=3.9\n",
    "```\n",
    "\n",
    "Then activate the newly created environment using the following command:\n",
    "```bash\n",
    "conda activate NetShare\n",
    "```\n",
    "\n",
    "Now clone the repositories to your machine under the same repository with this notebook and install them:\n",
    "```bash\n",
    "git clone https://github.com/sunnyyao666/NetShare\n",
    "pip3 install -e NetShare/\n",
    "\n",
    "git clone https://github.com/netsharecmu/SDMetrics_timeseries\n",
    "pip3 install -e SDMetrics_timeseries/\n",
    "```\n",
    "\n",
    "Finally, install Jupyter notebook in the new environment and start this notebook in the new environment.\n",
    "```bash\n",
    "conda install jupyter\n",
    "jupyter notebook\n",
    "```\n",
    "Your environment should be ready now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NetShare Preprocess Pipeline\n",
    "\n",
    "Before you can feed `PCAP` file data into NetShare to train a model/generate synthetic results, it needs to be preprocessed into the expected `CSV` input format. First we provide a generalized preprocess pipeline for converting pcap files into csv files for `Modbus` (and almost any other protocols), and also gives an example guide on how to use it.\n",
    "\n",
    "The preprocess guide will be provided by explaining the elements in the pipeline one by one in the order they appear in the pipeline and give examples. By following this guide you should be able to use the pipeline to convert `PCAP` files into `CSV` files. You can also skip some of the steps if you don't need them (e.g., if you already have a `Zeek` log file, you can skip the first two elements).\n",
    "\n",
    "Overview of the preprocess pipeline:\n",
    "\n",
    "![Preprocess pipeline](https://raw.githubusercontent.com/sunnyyao666/NetShare-Preprocess/main/flow_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "####  *.pcap\n",
    "\n",
    "Any packet capture file (dataset) that wants to be used as the input of NetShare.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Zeek\n",
    "\n",
    "Use Zeek as the process tool to parse `*.pcap` files to json log files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Installation\n",
    "\n",
    "Full installation instructions can be found [here](https://docs.zeek.org/en/lts/install.html).\n",
    "\n",
    "Here we provide the installation of `Zeek` on some systems. If you can't find your system here, you can follow the full installation instructions above and try to install it yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Installation on macOS\n",
    "\n",
    "If your system is macOS, `Zeek` can be installed using homebrew.\n",
    "\n",
    "First running the following command to install homebrew:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! /bin/bash -c \"$(curl -fsSL https://raw.githubusercontent.com/Homebrew/install/HEAD/install.sh)\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following command to verify installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-17T21:56:14.438721Z",
     "start_time": "2023-07-17T21:56:13.281598Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Homebrew 4.0.28\n",
      "Homebrew/homebrew-core (git revision 5c5075d96b1; last commit 2023-03-24)\n",
      "Homebrew/homebrew-cask (git revision ba10cd38d2; last commit 2023-03-25)\n"
     ]
    }
   ],
   "source": [
    "! brew --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then running the following command to install Xcode or the \"Command Line Tools\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! xcode-select --install"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the following command to verify installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-17T21:56:16.750957Z",
     "start_time": "2023-07-17T21:56:16.608042Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Library/Developer/CommandLineTools\r\n"
     ]
    }
   ],
   "source": [
    "! xcode-select -p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, use the following command to install Zeek:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! brew install zeek"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And use the following command to verify installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-17T21:56:19.361358Z",
     "start_time": "2023-07-17T21:56:19.074941Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zeek version 5.2.2\r\n"
     ]
    }
   ],
   "source": [
    "! zeek --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Installation on Ubuntu\n",
    "\n",
    "If your system is Ubuntu, you can install `Zeek` by following these instructions.\n",
    "\n",
    "First, run the following command to update the system with latest available packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo apt-get update -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run the following command to install all required dependencies of `Zeek`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo apt-get install curl gnupg2 wget -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because by default, `Zeek` is not included in the Ubuntu default repository, so you will need to add the Zeek repository to the system. Run the following command to download and add the Zeek GPG key. Notice that you may need to change the `20.04` in the following two commands to your own Ubuntu version."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -fsSL https://download.opensuse.org/repositories/security:zeek/xUbuntu_20.04/Release.key | gpg --dearmor | sudo tee /etc/apt/trusted.gpg.d/security_zeek.gpg > /dev/null"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run the following command to add the Zeek repository to APT:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! echo 'deb http://download.opensuse.org/repositories/security:/zeek/xUbuntu_20.04/ /' | sudo tee /etc/apt/sources.list.d/security:zeek.list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, run the following commands in the terminal to  update the repository cache and install `Zeek`. During the installation, you will be asked to provide some postfix settings. You can follow the instructions and choose the settings according to your need, or you can simply choose `No configuration`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! sudo apt-get update -y\n",
    "! sudo apt-get install zeek -y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now you need to add `Zeek` to the system path. You can do this by running the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-16T17:43:52.912272Z",
     "start_time": "2023-07-16T17:43:52.875366Z"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['PATH'] += ':/opt/zeek/bin'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also permanently add `Zeek` to the system path by modifying `~/.bashrc`.\n",
    "\n",
    "Now, use the following command to verify installation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-16T17:51:49.615623Z",
     "start_time": "2023-07-16T17:51:49.143132Z"
    }
   },
   "outputs": [],
   "source": [
    "! zeek --version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Usage\n",
    "\n",
    "Prepare the `PCAP` file that you want to parse. You can download an example by running this command:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-17T21:57:57.877174Z",
     "start_time": "2023-07-17T21:57:55.707448Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2023-07-19 01:51:34--  https://github.com/antoine-lemay/Modbus_dataset/raw/master/run8.pcap\n",
      "Resolving github.com (github.com)... 140.82.114.4\n",
      "Connecting to github.com (github.com)|140.82.114.4|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://raw.githubusercontent.com/antoine-lemay/Modbus_dataset/master/run8.pcap [following]\n",
      "--2023-07-19 01:51:34--  https://raw.githubusercontent.com/antoine-lemay/Modbus_dataset/master/run8.pcap\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 2606:50c0:8001::154, 2606:50c0:8000::154, 2606:50c0:8002::154, ...\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|2606:50c0:8001::154|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 7190064 (6.9M) [application/octet-stream]\n",
      "Saving to: ‘run8.pcap’\n",
      "\n",
      "run8.pcap           100%[===================>]   6.86M  32.9MB/s    in 0.2s    \n",
      "\n",
      "2023-07-19 01:51:35 (32.9 MB/s) - ‘run8.pcap’ saved [7190064/7190064]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "! wget https://github.com/antoine-lemay/Modbus_dataset/raw/master/run8.pcap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then change to the working directory, and run the following zeek command:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-16T18:05:26.838688Z",
     "start_time": "2023-07-16T18:05:22.310584Z"
    }
   },
   "outputs": [],
   "source": [
    "! mkdir zeek\n",
    "! cd zeek && zeek -C -r ../run8.pcap LogAscii::use_json=T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the command, `../run8.pcap` should be replaced with the desired `PCAP` file path.\n",
    "\n",
    "Option `-C` means to ignore invalid IP Checksums, `-r` means to parse an existing `PCAP` file, and `LogAscii::use_json=T` means to output results in `JSON` format (which is important, because the pipeline was designed to handle `JSON` log format). The results will be generated in the working directory. Other custom or extra scripts can also be added after the command to be loaded. Full instructions can be found [here](https://docs.zeek.org/en/master/quickstart.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### input file\n",
    "\n",
    "Input file of `parse2csv` function.\n",
    "\n",
    "Currently supported format:\n",
    "\n",
    "*   Output file from `Zeek` (`*.log`) in `JSON` format\n",
    "*   `CSV` file (`*.csv`)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### config.json\n",
    "\n",
    "A configuration file to indicate file path and fields that need to be extracted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Example\n",
    "\n",
    "The example config file is at `./modbus_config.json`.\n",
    "\n",
    "The content of the file is shown here:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"input_file\": {\n",
    "    \"path\": \"zeek/modbus.log\",\n",
    "    \"format\": \"zeek_json_log\"\n",
    "  },\n",
    "  \"output_file\": \"result.csv\",\n",
    "  \"fields\": {\n",
    "    \"timestamp\": [\n",
    "      {\n",
    "        \"name\": \"ts\",\n",
    "        \"parse\": \"second2micro\",\n",
    "        \"to\": \"time\",\n",
    "        \"format\": \"integer\",\n",
    "        \"encoding\": \"timestamp\"\n",
    "      }\n",
    "    ],\n",
    "    \"metadata\": [\n",
    "      {\n",
    "        \"name\": \"id.orig_h\",\n",
    "        \"parse\": \"ip_quad2int\",\n",
    "        \"to\": \"srcip\",\n",
    "        \"format\": \"integer\",\n",
    "        \"encoding\": \"bit\"\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"id.orig_p\",\n",
    "        \"to\": \"srcport\",\n",
    "        \"format\": \"integer\",\n",
    "        \"encoding\": \"word_port\"\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"id.resp_h\",\n",
    "        \"parse\": \"ip_quad2int\",\n",
    "        \"to\": \"dstip\",\n",
    "        \"format\": \"integer\",\n",
    "        \"abnormal\": true,\n",
    "        \"encoding\": \"bit\"\n",
    "      },\n",
    "      {\n",
    "        \"name\": \"id.resp_p\",\n",
    "        \"to\": \"dstport\",\n",
    "        \"format\": \"integer\",\n",
    "        \"encoding\": \"word_port\"\n",
    "      }\n",
    "    ],\n",
    "    \"timeseries\": [\n",
    "      {\n",
    "        \"name\": \"func\",\n",
    "        \"parse\": \"modbus_func2code\",\n",
    "        \"to\": \"funccode\",\n",
    "        \"format\": \"integer\",\n",
    "        \"encoding\": \"categorical\"\n",
    "      }\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Explanation\n",
    "\n",
    "* `input_file`: required, configuration of the input file.\n",
    "    * `path`: required, path to the input file.\n",
    "    * `format`: optional, format of the input file. Default: `zeek_log_json`. Supported choices:\n",
    "        * `zeek_log_json`: Output file from `Zeek` (*.log) in `JSON` format;\n",
    "        * `csv`: `CSV` file (*.csv).\n",
    "* `output_file`: optional, path to the output `CSV` file. Default: `./result.csv`.\n",
    "* `fields`: required, fields in the input file that need to be extracted. Each field should be put under one of the three categories: `timestamp`, `metadata` or `timeseries`.\n",
    "    * `name`: required, key of the field that appeared in the input file.\n",
    "    * `to`: optional, the header of the field in the output `CSV` file. Default: the same as `name`.\n",
    "    * `parse`: optional, name of the parsing function to be applied on the field defined in `pasre_func.py`. Must ensure that the function exist in `parse_func.py`. Default: `None`.\n",
    "    * `format`: optional, output format of the field (after parsing). Default: `str`.\n",
    "    * `abnormal`: optional, if this field needs abnormal handling. Default: `false`.\n",
    "    * `encoding`: required, the encoding method to be used on the field when training NetShare model. Please refer to the `NetShare Synthetic Data Generation`/`Usage` section for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### parse_func.py\n",
    "\n",
    "This is a python file that contains customized parsing functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Example\n",
    "\n",
    "Here are three example parsing functions. You can use them as you needed.\n",
    "\n",
    "```python\n",
    "def second2micro(second):\n",
    "    \"\"\"\n",
    "    :param second: Time in second\n",
    "    :return: Time in microsecond\n",
    "    \"\"\"\n",
    "    return second * 1000000\n",
    "\n",
    "\n",
    "def ip_quad2int(ip_quad_string):\n",
    "    \"\"\"\n",
    "    Parse a dotted-quad string IP address (e.g., 192.168.0.0) to a unsigned int\n",
    "    by first convert the IP address back to 32-bit binary format and then covert the binary to decimal.\n",
    "\n",
    "    :param ip_quad_string: IP address in dotted-quad string format (e.g., 192.168.0.0)\n",
    "    :return: Converted unsigned int result\n",
    "    \"\"\"\n",
    "    # Bytes format of the IP address (192.168.0.0 -> b'\\xc0\\xa8\\x00\\x00)\n",
    "    ip_bytes = socket.inet_aton(ip_quad_string)\n",
    "    # Convert the bytes to a decimal with big endian byte order\n",
    "    return int.from_bytes(ip_bytes, 'big')\n",
    "\n",
    "\n",
    "def modbus_func2code(func_name):\n",
    "    \"\"\"\n",
    "    :param func_name: Modbus function name\n",
    "    :return: Corresponding modbus function code\n",
    "    \"\"\"\n",
    "    if func_name == 'READ_COILS':\n",
    "        return 1\n",
    "    elif func_name == 'READ_DISCRETE_INPUTS':\n",
    "        return 2\n",
    "    elif func_name == 'READ_HOLDING_REGISTERS':\n",
    "        return 3\n",
    "    elif func_name == 'READ_INPUT_REGISTERS':\n",
    "        return 4\n",
    "    elif func_name == 'WRITE_SINGLE_COIL':\n",
    "        return 5\n",
    "    elif func_name == 'WRITE_SINGLE_REGISTER':\n",
    "        return 6\n",
    "    else:\n",
    "        return 0\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Explanation\n",
    "\n",
    "* Each parsing function should take one parameter as input (the original field value) and return the parsed value.\n",
    "\n",
    "* The returned value should be consistent with the output `format` (see previous section).\n",
    "\n",
    "* Each function name that appears in the `config.json` file must be implemented here.\n",
    "\n",
    "* If you need to implement more customized parsing functions, just add functions as you needed in `parse_func.py` and put the names in the `parse` field of the config file as explained above.\n",
    "\n",
    "* Abnormal handling:\n",
    "    * Definition of abnormal:\n",
    "        * Value of numbers less than 0;\n",
    "        * Empty field.\n",
    "    * If detected abnormal, the value of the field will be changed into `unavailable` (if output format is `str`) or `0` (if output format is number);\n",
    "    * If not, the original value will not be changed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### parse2csv.py\n",
    "\n",
    "This is a python file that contains a python function `parse2csv`, which requires a parameter indicating the path to the config file.\n",
    "\n",
    "First, you need to install the following dependency to read the config file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install config_io"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then run the following function with the path to the config file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-07-16T18:30:33.460413Z",
     "start_time": "2023-07-16T18:30:30.889809Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'result.csv'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from parse2csv import parse_to_csv\n",
    "parse_to_csv('modbus_config.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to see the result csv at the output path.\n",
    "\n",
    "By calling this function with appropriate configuration, it will parse the input file according to the configration and output it as a `CSV` file, which can be directly used as the input of the NetShare system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NetShare Synthetic Data Generation\n",
    "\n",
    "Here, we will provide guides on running the NetShare on your local machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Usage\n",
    "\n",
    "First use the following commands to move the generated result from the preprocess pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir NetShare/traces/modbus\n",
    "! mv result.csv NetShare/traces/modbus/modbus.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The working directory is under `NetShare/examples/modbus`. In the working directory, `driver.py` is the entrance Python file to start the whole training pipeline; the other file is a config file. Here is an example:\n",
    "\n",
    "```json\n",
    "{\n",
    "    \"global_config\": {\n",
    "        \"original_data_file\": \"../../traces/modbus/modbus.csv\",\n",
    "        \"overwrite\": true,\n",
    "        \"dataset_type\": \"pcap\",\n",
    "        \"n_chunks\": 1,\n",
    "        \"dp\": false\n",
    "    },\n",
    "    \"default\": \"single_event_per_row.json\",\n",
    "    \"pre_post_processor\": {\n",
    "        \"class\": \"NetsharePrePostProcessor\",\n",
    "        \"config\": {\n",
    "            \"timestamp\": {\n",
    "                \"column\": \"time\",\n",
    "                \"generation\": true,\n",
    "                \"encoding\": \"interarrival\",\n",
    "                \"normalization\": \"ZERO_ONE\"\n",
    "            },\n",
    "            \"word2vec\": {\n",
    "                \"vec_size\": 10,\n",
    "                \"model_name\": \"word2vec_vecSize\",\n",
    "                \"annoy_n_trees\": 100,\n",
    "                \"pretrain_model_path\": null\n",
    "            },\n",
    "            \"metadata\": [\n",
    "                {\n",
    "                    \"column\": \"srcip\",\n",
    "                    \"type\": \"integer\",\n",
    "                    \"encoding\": \"bit\",\n",
    "                    \"n_bits\": 32,\n",
    "                    \"categorical_mapping\": false\n",
    "                },\n",
    "                {\n",
    "                    \"column\": \"dstip\",\n",
    "                    \"type\": \"integer\",\n",
    "                    \"encoding\": \"bit\",\n",
    "                    \"n_bits\": 32,\n",
    "                    \"categorical_mapping\": false\n",
    "                },\n",
    "                {\n",
    "                    \"column\": \"srcport\",\n",
    "                    \"type\": \"integer\",\n",
    "                    \"encoding\": \"word2vec_port\"\n",
    "                },\n",
    "                {\n",
    "                    \"column\": \"dstport\",\n",
    "                    \"type\": \"integer\",\n",
    "                    \"encoding\": \"word2vec_port\"\n",
    "                }\n",
    "            ],\n",
    "            \"timeseries\": [\n",
    "                {\n",
    "                    \"column\": \"funccode\",\n",
    "                    \"type\": \"integer\",\n",
    "                    \"encoding\": \"categorical\",\n",
    "                    \"choices\": [\n",
    "                        0,\n",
    "                        1,\n",
    "                        2,\n",
    "                        3,\n",
    "                        4,\n",
    "                        5,\n",
    "                        6\n",
    "                    ]\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"class\": \"DoppelGANgerTorchModel\",\n",
    "        \"config\": {\n",
    "            \"batch_size\": 100,\n",
    "            \"sample_len\": [\n",
    "                10\n",
    "            ],\n",
    "            \"sample_len_expand\": true,\n",
    "            \"epochs\": 4,\n",
    "            \"extra_checkpoint_freq\": 1,\n",
    "            \"epoch_checkpoint_freq\": 5\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "The most important config in the file is how to encode the field under `metadata` and `timeseries`. Currently, NetShare supports the following field configs:\n",
    "\n",
    "* Bit field (encoded as bit strings) e.g.,\n",
    "```json\n",
    "{\n",
    "    \"column\": \"srcip\",\n",
    "    \"type\": \"integer\",\n",
    "    \"encoding\": \"bit\",\n",
    "    \"n_bits\": 32\n",
    "}\n",
    "```\n",
    "\n",
    "* Word2Vec field (encoded as Word2Vec vectors), e.g.,\n",
    "```json\n",
    "{\n",
    "    \"column\": \"srcport\",\n",
    "    \"type\": \"integer\",\n",
    "    \"encoding\": \"word2vec_port\"\n",
    "}\n",
    "```\n",
    "\n",
    "* Categorical field (encoded as one-hot encoding), e.g.,\n",
    "```json\n",
    "{\n",
    "    \"column\": \"type\",\n",
    "    \"type\": \"string\",\n",
    "    \"encoding\": \"categorical\"\n",
    "}\n",
    "```\n",
    "Here type can be `string` or `integer`.\n",
    "\n",
    "* Continuous field, e.g.,\n",
    "```json\n",
    "{\n",
    "    \"column\": \"pkt\",\n",
    "    \"type\": \"float\",\n",
    "    \"normalization\": \"ZERO_ONE\",\n",
    "    \"log1p_norm\": true\n",
    "}\n",
    "```\n",
    "\n",
    "You should choose to config your fields as needed according to the \"nearest match\".\n",
    "\n",
    "Now change to the working directory and run the following command to start the train and generation of NetShare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: No IPv4 address found on en2 !\n",
      "WARNING: No IPv4 address found on awdl0 !\n",
      "WARNING: more No IPv4 address found on llw0 !\n",
      "[KeOps] Warning : Cuda libraries were not detected on the system ; using cpu only mode\n",
      "Ray is disabled\n",
      "NetsharePrePostProcessor._pre_process\n",
      "../../traces/modbus/modbus.csv\n",
      "dataset type: pcap\n",
      "metadata cols: ['srcip', 'dstip', 'srcport', 'dstport']\n",
      "word2vec cols: ['srcport', 'dstport']\n",
      "Training Word2Vec model from scratch...\n",
      "07/19/2023 01:55:16:WARNING:consider setting layer size to a multiple of 4 for greater performance\n",
      "07/19/2023 01:55:18:WARNING:under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n",
      "Word2Vec model is saved at ../../results/modbus/pre_processed_data/word2vec_vecSize_10.model\n",
      "Building annoy dictionary word2vec...\n",
      "{'port': ['srcport', 'dstport']}\n",
      "Finish building Angular trees...\n",
      "metadata fields: ['srcip', 'dstip', 'srcport', 'dstport']\n",
      "timeseries fields: ['funccode']\n",
      "Using fixed_time\n",
      "1\n",
      "Chunk_id: 0, # of pkts/records: 13022\n",
      "df_chunk_cnt_validation: 13022\n",
      "Chunk time: 3560.024766 seconds\n",
      "compute flowkey-chunk list from scratch...\n",
      "processing chunk 1/1, # of flows: 6511\n",
      "# of total flows: 6511\n",
      "# of total flows (sanity check): 6511\n",
      "# of flows cross chunk (of total flows): 0 (0.0%)\n",
      "# of non-continuous flows: 0\n",
      "chunk_id: 0, max_flow_len: 2\n",
      "global max flow len: 2\n",
      "Top 10 per-chunk flow length: [2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n",
      "0it [00:00, ?it/s]\n",
      "Chunk_id: 0\n",
      "Before truncation, df_per_chunk: (13022, 7)\n",
      "/Users/sunnyyao666/PycharmProjects/yht/NetShare/netshare/pre_post_processors/netshare/preprocess_helper.py:216: FutureWarning: Not prepending group keys to the result index of transform-like apply. In the future, the group keys will be included in the index, regardless of whether the applied function returns a like-indexed object.\n",
      "To preserve the previous behavior, use\n",
      "\n",
      "\t>>> .groupby(..., group_keys=False)\n",
      "\n",
      "To adopt the future behavior and silence this warning, use \n",
      "\n",
      "\t>>> .groupby(..., group_keys=True)\n",
      "  processed = grouped.apply(process_group)\n",
      "After truncation, df_per_chunk: (13022, 7)\n",
      "df_per_chunk: (13022, 162)\n",
      "\n",
      "  0%|          | 0/6511 [00:00<?, ?it/s]\u001B[A\n",
      "  0%|          | 1/6511 [00:02<4:01:49,  2.23s/it]\u001B[A\n",
      "  1%|          | 51/6511 [00:02<03:31, 30.52it/s] \u001B[A\n",
      "  2%|1         | 98/6511 [00:02<01:38, 65.22it/s]\u001B[A\n",
      "  2%|2         | 138/6511 [00:02<01:04, 99.13it/s]\u001B[A\n",
      "  3%|3         | 201/6511 [00:02<00:37, 166.19it/s]\u001B[A\n",
      "  4%|4         | 261/6511 [00:02<00:27, 230.04it/s]\u001B[A\n",
      "  5%|4         | 320/6511 [00:02<00:21, 293.61it/s]\u001B[A\n",
      "  6%|5         | 387/6511 [00:02<00:16, 369.36it/s]\u001B[A\n",
      "  7%|7         | 460/6511 [00:03<00:13, 449.83it/s]\u001B[A\n",
      "  8%|8         | 535/6511 [00:03<00:11, 521.93it/s]\u001B[A\n",
      "  9%|9         | 609/6511 [00:03<00:10, 576.61it/s]\u001B[A\n",
      " 11%|#         | 685/6511 [00:03<00:09, 625.54it/s]\u001B[A\n",
      " 12%|#1        | 767/6511 [00:03<00:08, 677.92it/s]\u001B[A\n",
      " 13%|#2        | 846/6511 [00:03<00:08, 707.74it/s]\u001B[A\n",
      " 14%|#4        | 931/6511 [00:03<00:07, 747.59it/s]\u001B[A\n",
      " 16%|#5        | 1010/6511 [00:03<00:07, 722.83it/s]\u001B[A\n",
      " 17%|#6        | 1085/6511 [00:03<00:07, 705.02it/s]\u001B[A\n",
      " 18%|#7        | 1158/6511 [00:03<00:07, 696.26it/s]\u001B[A\n",
      " 19%|#8        | 1229/6511 [00:04<00:07, 683.82it/s]\u001B[A\n",
      " 20%|##        | 1305/6511 [00:04<00:07, 703.22it/s]\u001B[A\n",
      " 21%|##1       | 1392/6511 [00:04<00:06, 737.84it/s]\u001B[A\n",
      " 23%|##2       | 1467/6511 [00:04<00:06, 731.00it/s]\u001B[A\n",
      " 24%|##3       | 1541/6511 [00:04<00:06, 722.02it/s]\u001B[A\n",
      " 25%|##4       | 1614/6511 [00:04<00:07, 671.84it/s]\u001B[A\n",
      " 26%|##5       | 1685/6511 [00:04<00:07, 681.67it/s]\u001B[A\n",
      " 27%|##6       | 1755/6511 [00:04<00:06, 685.91it/s]\u001B[A\n",
      " 28%|##8       | 1827/6511 [00:04<00:06, 693.78it/s]\u001B[A\n",
      " 29%|##9       | 1897/6511 [00:05<00:06, 673.72it/s]\u001B[A\n",
      " 30%|###       | 1971/6511 [00:05<00:06, 691.31it/s]\u001B[A\n",
      " 31%|###1      | 2041/6511 [00:05<00:06, 688.24it/s]\u001B[A\n",
      " 32%|###2      | 2113/6511 [00:05<00:06, 694.52it/s]\u001B[A\n",
      " 34%|###3      | 2196/6511 [00:05<00:05, 733.03it/s]\u001B[A\n",
      " 35%|###4      | 2270/6511 [00:05<00:05, 734.95it/s]\u001B[A\n",
      " 36%|###6      | 2345/6511 [00:05<00:05, 733.56it/s]\u001B[A\n",
      " 37%|###7      | 2426/6511 [00:05<00:05, 754.91it/s]\u001B[A\n",
      " 38%|###8      | 2502/6511 [00:05<00:05, 732.94it/s]\u001B[A\n",
      " 40%|###9      | 2578/6511 [00:05<00:05, 739.15it/s]\u001B[A\n",
      " 41%|####      | 2653/6511 [00:06<00:05, 739.45it/s]\u001B[A\n",
      " 42%|####1     | 2732/6511 [00:06<00:05, 751.60it/s]\u001B[A\n",
      " 43%|####3     | 2808/6511 [00:06<00:05, 731.65it/s]\u001B[A\n",
      " 44%|####4     | 2887/6511 [00:06<00:04, 746.42it/s]\u001B[A\n",
      " 45%|####5     | 2962/6511 [00:06<00:04, 742.11it/s]\u001B[A\n",
      " 47%|####6     | 3037/6511 [00:06<00:04, 732.43it/s]\u001B[A\n",
      " 48%|####7     | 3121/6511 [00:06<00:04, 762.16it/s]\u001B[A\n",
      " 49%|####9     | 3198/6511 [00:06<00:04, 729.14it/s]\u001B[A\n",
      " 50%|#####     | 3280/6511 [00:06<00:04, 754.34it/s]\u001B[A\n",
      " 52%|#####1    | 3356/6511 [00:07<00:04, 746.07it/s]\u001B[A\n",
      " 53%|#####2    | 3438/6511 [00:07<00:04, 761.59it/s]\u001B[A\n",
      " 54%|#####3    | 3515/6511 [00:07<00:04, 704.49it/s]\u001B[A\n",
      " 55%|#####5    | 3587/6511 [00:07<00:04, 679.37it/s]\u001B[A\n",
      " 56%|#####6    | 3656/6511 [00:07<00:04, 642.69it/s]\u001B[A\n",
      " 57%|#####7    | 3721/6511 [00:07<00:04, 626.89it/s]\u001B[A\n",
      " 58%|#####8    | 3785/6511 [00:07<00:04, 616.55it/s]\u001B[A\n",
      " 59%|#####9    | 3847/6511 [00:07<00:04, 603.81it/s]\u001B[A\n",
      " 60%|######    | 3908/6511 [00:07<00:05, 520.31it/s]\u001B[A\n",
      " 61%|######1   | 3988/6511 [00:08<00:04, 588.55it/s]\u001B[A\n",
      " 62%|######2   | 4057/6511 [00:08<00:03, 614.97it/s]\u001B[A\n",
      " 63%|######3   | 4121/6511 [00:08<00:04, 563.76it/s]\u001B[A\n",
      " 64%|######4   | 4198/6511 [00:08<00:03, 610.34it/s]\u001B[A\n",
      " 66%|######5   | 4265/6511 [00:08<00:03, 618.37it/s]\u001B[A\n",
      " 66%|######6   | 4329/6511 [00:08<00:03, 621.69it/s]\u001B[A\n",
      " 67%|######7   | 4393/6511 [00:08<00:03, 594.78it/s]\u001B[A\n",
      " 69%|######8   | 4461/6511 [00:08<00:03, 616.89it/s]\u001B[A\n",
      " 70%|######9   | 4529/6511 [00:08<00:03, 632.92it/s]\u001B[A\n",
      " 71%|#######   | 4593/6511 [00:09<00:03, 605.34it/s]\u001B[A\n",
      " 71%|#######1  | 4655/6511 [00:09<00:03, 482.09it/s]\u001B[A\n",
      " 72%|#######2  | 4719/6511 [00:09<00:03, 519.24it/s]\u001B[A\n",
      " 73%|#######3  | 4782/6511 [00:09<00:03, 547.41it/s]\u001B[A\n",
      " 74%|#######4  | 4846/6511 [00:09<00:02, 572.15it/s]\u001B[A\n",
      " 75%|#######5  | 4911/6511 [00:09<00:02, 592.28it/s]\u001B[A\n",
      " 76%|#######6  | 4973/6511 [00:09<00:02, 590.29it/s]\u001B[A\n",
      " 77%|#######7  | 5041/6511 [00:09<00:02, 614.79it/s]\u001B[A\n",
      " 78%|#######8  | 5106/6511 [00:09<00:02, 616.48it/s]\u001B[A\n",
      " 79%|#######9  | 5169/6511 [00:10<00:02, 605.78it/s]\u001B[A\n",
      " 80%|########  | 5231/6511 [00:10<00:02, 606.39it/s]\u001B[A\n",
      " 82%|########1 | 5314/6511 [00:10<00:01, 668.62it/s]\u001B[A\n",
      " 83%|########2 | 5382/6511 [00:10<00:01, 663.96it/s]\u001B[A\n",
      " 84%|########3 | 5466/6511 [00:10<00:01, 714.37it/s]\u001B[A\n",
      " 85%|########5 | 5541/6511 [00:10<00:01, 724.29it/s]\u001B[A\n",
      " 86%|########6 | 5614/6511 [00:10<00:01, 722.08it/s]\u001B[A\n",
      " 87%|########7 | 5694/6511 [00:10<00:01, 743.09it/s]\u001B[A\n",
      " 89%|########8 | 5772/6511 [00:10<00:00, 752.99it/s]\u001B[A\n",
      " 90%|########9 | 5848/6511 [00:10<00:00, 742.96it/s]\u001B[A\n",
      " 91%|#########1| 5931/6511 [00:11<00:00, 766.78it/s]\u001B[A\n",
      " 92%|#########2| 6008/6511 [00:11<00:00, 758.19it/s]\u001B[A\n",
      " 93%|#########3| 6084/6511 [00:11<00:00, 743.94it/s]\u001B[A\n",
      " 95%|#########4| 6166/6511 [00:11<00:00, 764.11it/s]\u001B[A\n",
      " 96%|#########5| 6244/6511 [00:11<00:00, 767.57it/s]\u001B[A\n",
      " 97%|#########7| 6321/6511 [00:11<00:00, 755.44it/s]\u001B[A\n",
      " 98%|#########8| 6403/6511 [00:11<00:00, 774.12it/s]\u001B[A\n",
      "100%|##########| 6511/6511 [00:11<00:00, 549.27it/s]\u001B[A\n",
      "data_attribute: (6511, 149), 0.007761112GB in memory\n",
      "data_feature: (6511, 2, 8), 0.000833408GB in memory\n",
      "data_gen_flag: (6511, 2), 0.000104176GB in memory\n",
      "1it [00:26, 27.00s/it]\n",
      "NetShareManager._train\n",
      "Number of valid chunks: 1\n",
      "Number of configurations after expanded: 1\n",
      "[{'dp_noise_multiplier': None, 'dp': False, 'pretrain': True, 'config_ids': [0]}]\n",
      "Config group 0: DP: False, pretrain: True\n",
      "Start launching chunk0 experiments...\n",
      "DoppelGANgerTorchModel._train\n",
      "Currently training with config: {'overwrite': True, 'original_data_file': '../../traces/modbus/modbus.csv', 'dataset_type': 'pcap', 'n_chunks': 1, 'dp': False, 'allowed_data_types': ['ip_string', 'integer', 'float', 'string'], 'allowed_data_encodings': ['categorical', 'bit', 'word2vec_port', 'word2vec_proto'], 'pretrain_dir': '../../results/modbus/models/chunkid-0/sample_len-10/checkpoint/epoch_id-4.pt', 'skip_chunk0_train': False, 'pretrain_non_dp': True, 'pretrain_non_dp_reduce_time': 4.0, 'pretrain_dp': False, 'run': 0, 'batch_size': 100, 'sample_len': 10, 'sample_len_expand': True, 'iteration': 200000, 'vis_freq': 100000, 'vis_num_sample': 5, 'd_rounds': 5, 'g_rounds': 1, 'num_packing': 1, 'noise': True, 'attr_noise_type': 'normal', 'feature_noise_type': 'normal', 'rnn_mlp_num_layers': 0, 'feed_back': False, 'g_lr': 0.0001, 'g_beta1': 0.5, 'd_lr': 0.0001, 'd_beta1': 0.5, 'd_gp_coe': 10.0, 'adam_eps': 1e-08, 'adam_amsgrad': False, 'generator_feature_num_layers': 1, 'generator_feature_num_units': 100, 'generator_attribute_num_layers': 5, 'generator_attribute_num_units': 512, 'discriminator_num_layers': 5, 'discriminator_num_units': 512, 'initial_state': 'random', 'leaky_relu': False, 'attr_d_lr': 0.0001, 'attr_d_beta1': 0.5, 'attr_d_gp_coe': 10.0, 'g_attr_d_coe': 1.0, 'attr_discriminator_num_layers': 5, 'attr_discriminator_num_units': 512, 'use_attr_discriminator': True, 'self_norm': False, 'fix_feature_network': False, 'debug': False, 'combined_disc': True, 'use_gt_lengths': False, 'use_uniform_lengths': False, 'num_cores': None, 'sn_mode': None, 'scale': 1.0, 'extra_checkpoint_freq': 1, 'epoch_checkpoint_freq': 5, 'dp_noise_multiplier': None, 'dp_l2_norm_clip': None, 'use_adaptive_rolling': False, 'attribute_latent_dim': 5, 'feature_latent_dim': 5, 'epochs': 5, 'dataset': '../../results/modbus/pre_processed_data/chunkid-0', 'dataset_expand': True, 'sub_result_folder': 'chunkid-0/sample_len-10', 'result_folder': '../../results/modbus/models/chunkid-0/sample_len-10', 'restore': False}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]WARNING: No IPv4 address found on en2 !\n",
      "WARNING: No IPv4 address found on en2 !\n",
      "WARNING: No IPv4 address found on awdl0 !\n",
      "WARNING: No IPv4 address found on awdl0 !\n",
      "WARNING: more No IPv4 address found on llw0 !\n",
      "WARNING: more No IPv4 address found on llw0 !\n",
      "[KeOps] Warning : Cuda libraries were not detected on the system ; using cpu only mode\n",
      "[KeOps] Warning : Cuda libraries were not detected on the system ; using cpu only mode\n",
      "100%|##########| 5/5 [02:59<00:00, 35.96s/it]\n",
      "Finish launching chunk0 experiments ...\n",
      "Number of valid chunks: 1\n",
      "Number of configurations after expanded: 1\n",
      "Start generating attributes ...\n",
      "DoppelGANgerTorchModel._generate\n",
      "Currently generating with config: {'overwrite': True, 'original_data_file': '../../traces/modbus/modbus.csv', 'dataset_type': 'pcap', 'n_chunks': 1, 'dp': False, 'allowed_data_types': ['ip_string', 'integer', 'float', 'string'], 'allowed_data_encodings': ['categorical', 'bit', 'word2vec_port', 'word2vec_proto'], 'pretrain_dir': '../../results/modbus/models/chunkid-0/sample_len-10/checkpoint/epoch_id-4.pt', 'skip_chunk0_train': False, 'pretrain_non_dp': True, 'pretrain_non_dp_reduce_time': 4.0, 'pretrain_dp': False, 'run': 0, 'batch_size': 100, 'sample_len': 10, 'sample_len_expand': True, 'iteration': 200000, 'vis_freq': 100000, 'vis_num_sample': 5, 'd_rounds': 5, 'g_rounds': 1, 'num_packing': 1, 'noise': True, 'attr_noise_type': 'normal', 'feature_noise_type': 'normal', 'rnn_mlp_num_layers': 0, 'feed_back': False, 'g_lr': 0.0001, 'g_beta1': 0.5, 'd_lr': 0.0001, 'd_beta1': 0.5, 'd_gp_coe': 10.0, 'adam_eps': 1e-08, 'adam_amsgrad': False, 'generator_feature_num_layers': 1, 'generator_feature_num_units': 100, 'generator_attribute_num_layers': 5, 'generator_attribute_num_units': 512, 'discriminator_num_layers': 5, 'discriminator_num_units': 512, 'initial_state': 'random', 'leaky_relu': False, 'attr_d_lr': 0.0001, 'attr_d_beta1': 0.5, 'attr_d_gp_coe': 10.0, 'g_attr_d_coe': 1.0, 'attr_discriminator_num_layers': 5, 'attr_discriminator_num_units': 512, 'use_attr_discriminator': True, 'self_norm': False, 'fix_feature_network': False, 'debug': False, 'combined_disc': True, 'use_gt_lengths': False, 'use_uniform_lengths': False, 'num_cores': None, 'sn_mode': None, 'scale': 1.0, 'extra_checkpoint_freq': 1, 'epoch_checkpoint_freq': 5, 'dp_noise_multiplier': None, 'dp_l2_norm_clip': None, 'use_adaptive_rolling': False, 'attribute_latent_dim': 5, 'feature_latent_dim': 5, 'epochs': 5, 'dataset': '../../results/modbus/pre_processed_data/chunkid-0', 'dataset_expand': True, 'sub_result_folder': 'chunkid-0/sample_len-10', 'result_folder': '../../results/modbus/models/chunkid-0/sample_len-10', 'restore': False, 'chunk_id': 0, 'eval_root_folder': '../../results/modbus/generated_data/sample_len-10', 'given_data_attribute_flag': False}\n",
      "Generating w/o given data attribute!\n",
      "Processing epoch_id: 4\n",
      "generate 1-th sample from epoch_id-4\n",
      "Finished loading\n",
      "Finish generating features given attributes ...\n",
      "NetsharePrePostProcessor._post_process\n",
      "100%|##########| 1/1 [00:00<00:00,  1.09it/s]\n",
      "Config group #0: {'dp_noise_multiplier': None, 'dp': False, 'pretrain': True, 'config_ids': [0]}\n",
      "Chunk_id: 0, # of syn dfs: 1, best_syndf: epoch_id-4.csv\n",
      "Average truncation ratio: 0.0\n",
      "Big syndf shape: (7376, 6)\n",
      "\n",
      "Aggregated final dataset syndf\n",
      "None (7376, 6)\n",
      "best_syn_df filename: ../../results/modbus/post_processed_data/syn_df,dp_noise_multiplier-None,truncate-per_chunk,id-1.csv\n",
      "Generated data is at ../../results/modbus/post_processed_data\n",
      "The filename with the largest ID is: syn_df,dp_noise_multiplier-None,truncate-per_chunk,id-1.csv\n",
      "/Users/sunnyyao666/PycharmProjects/yht/SDMetrics_timeseries/sdmetrics/reports/utils.py:267: UserWarning:\n",
      "\n",
      "Real or synthetic column session_length is a constant list. Not generating plots.\n",
      "\n",
      "Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      "07/19/2023 01:58:53:INFO:Dash is running on http://127.0.0.1:8050/\n",
      "\n",
      " * Serving Flask app 'sdmetrics.reports.timeseries.quality_report'\n",
      " * Debug mode: off\n",
      "07/19/2023 01:58:53:INFO:\u001B[31m\u001B[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001B[0m\n",
      " * Running on http://127.0.0.1:8050\n",
      "07/19/2023 01:58:53:INFO:\u001B[33mPress CTRL+C to quit\u001B[0m\n",
      "07/19/2023 01:59:07:INFO:127.0.0.1 - - [19/Jul/2023 01:59:07] \"GET / HTTP/1.1\" 200 -\n",
      "07/19/2023 01:59:07:INFO:127.0.0.1 - - [19/Jul/2023 01:59:07] \"GET /_dash-dependencies HTTP/1.1\" 200 -\n",
      "07/19/2023 01:59:07:INFO:127.0.0.1 - - [19/Jul/2023 01:59:07] \"GET /_dash-layout HTTP/1.1\" 200 -\n",
      "07/19/2023 01:59:08:INFO:127.0.0.1 - - [19/Jul/2023 01:59:08] \"\u001B[36mGET /_dash-component-suites/dash/dcc/async-graph.js HTTP/1.1\u001B[0m\" 304 -\n",
      "07/19/2023 01:59:08:INFO:127.0.0.1 - - [19/Jul/2023 01:59:08] \"GET /_dash-component-suites/dash/dcc/async-plotlyjs.js HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "! cd NetShare/examples/modbus && python driver.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The results can be viewed under `NetShare/results/modbus` and in your browser."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
